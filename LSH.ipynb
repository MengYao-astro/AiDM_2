{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AiDM Assignment 2 Group 81\n",
    "### Meng Yao (yao@strw.leidenuniv.nl) and Michael Keim (keim@strw.leidenuniv.nl)\n",
    "##  Locality Sensitive Hashing for Netflix Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "start = time.time()\n",
    "np.random.seed(seed=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on in our code we will need to be able to calculate similarities. Here is where we define functions to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list(data,n):\n",
    "    '''\n",
    "    The function is to generate a list that contains sets for users. Each set is the watched movies for one user.\n",
    "    data :\n",
    "    n : persent of dataset will be considered\n",
    "    '''\n",
    "    data_n_persent = data[0:int(len(data_origin)*n),:] # Drag out n persent from the entire dataset.\n",
    "    user_movie_list = [[] for _ in range(len(set(data_n_persent[:,0])))] \n",
    "    # Prepare a list of sets.\n",
    "    # Every set represent a user  \n",
    "    for i in range(len(data_n_persent)):\n",
    "        user_movie_list[data_n_persent[i,0]].append(data_n_persent[i,1])\n",
    "    for j in range(len(user_movie_list)):\n",
    "        user_movie_list[j] = set(user_movie_list[j]) # convert every user list to a set\n",
    "    return user_movie_list  # will return the whole watched movies set for every user in only one list.\n",
    "def simi(pair,set_list):\n",
    "    a = set_list[pair[0]]\n",
    "    b = set_list[pair[1]]\n",
    "    intersection = a & b # Use logical and to achieve the intersection\n",
    "    n_intersection = len(intersection)\n",
    "    similarity = n_intersection / (len(a) + len(b) - n_intersection) # Calculate sim\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_list_total = generate_list(data_origin,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load Data\n",
    "First we read in the data of users (column 1) and the movies they rated (column 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_origin = np.load('../user_movie.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shingling\n",
    "In the interest of memory conservation (only stoing non-zero elements) and efficient rearrangement, we store the data as ones in indicies according to their user ID (column) and rated movies (rows, 1 if rated 0 if not). This is so that it will be easy to tell which users rated the same movies using LSH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "users_movies = csc_matrix((np.ones(len(data_origin)), (data_origin[:,1], data_origin[:,0])),\\\n",
    "                          shape=(np.max(data_origin[:,1])+1, np.max(data_origin[:,0])+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will need to find the first nonzero entry for every column in this sparse matrix (re-arranged). Let's see what this is for the first user in the original arrangement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 "
     ]
    }
   ],
   "source": [
    "first_nonzero = np.array((users_movies!=0).argmax(axis=0))[0]\n",
    "# Now test for the first user\n",
    "for i in range(first_nonzero[0]+1):\n",
    "    print(int(users_movies[i,0]), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signature Matrix\n",
    "Now we create a random permutations of movie indicies and rearrange our matrix to match. The first non-zero element in the new arrangement will be an element in the user's signature. We will start using 50 permutations, so that our signatures for each user are of length 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "signature_length = 50\n",
    "signature_matrix = np.zeros((signature_length, np.max(data_origin[:,0])+1))\n",
    "permutation_indicies = np.arange(np.max(data_origin[:,1])+1)\n",
    "# We create a copy of the sparse matrix which we will re-arrange\n",
    "users_movies_copy = np.copy(users_movies)\n",
    "for i in range(signature_length):\n",
    "    # We shuffle the permutations, thereby creating new hash functions\n",
    "    np.random.shuffle(permutation_indicies)\n",
    "    # We re-arrange the copy\n",
    "    users_movies_copy = users_movies[permutation_indicies,:]\n",
    "    # We find the first non-zero entry\n",
    "    signature_matrix[i,:] = (users_movies_copy!=0).argmax(axis=0)\n",
    "print('It took', int(time.time() - tic), 'seconds to make the signature matrix.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test to make sure we made our signture matrix correctly. For the last signature in the matrix for the first user, is the index actually the first non-zero entry in the permutation_indicies arrangement?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 "
     ]
    }
   ],
   "source": [
    "for j in permutation_indicies[1+np.arange(int(signature_matrix[signature_length-1,0]))]:\n",
    "    print(int(users_movies[j,0]), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, that is indeed the first non-zero entry in this ordering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Partition Into Bands\n",
    "Next we cut the signature matrix into 5 bands (to start),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_to_bucket(bands,sig_M):\n",
    "    rows = int(len(sig_M) / bands)\n",
    "    possible_sig_max = 17749\n",
    "    weight = np.arange(1,rows+1,1)\n",
    "    bucket_index_max = (1+rows)*rows/2 * possible_sig_max\n",
    "    candidate_list = [[[] for _ in range(int(bucket_index_max))] for _ in range(bands)]\n",
    "    candidates = []\n",
    "    for i in range(bands):\n",
    "        for j in range(len(sig_M[0])):\n",
    "            hash_wsum = sum(np.array(sig_M[int(i * rows):int((i+1)*rows),j]) * weight)\n",
    "            candidate_list[i][int(hash_wsum)].append(j)\n",
    "        candidates.append([x for x in candidate_list[i] if x])\n",
    "    return candidates\n",
    "\n",
    "def bucket_to_simi(candidates, Threshold ,start_time):\n",
    "    pairs = []\n",
    "    similarities = []\n",
    "    similar_users = []\n",
    "    for band in candidates:\n",
    "        for bucket in band:\n",
    "            for i in range(len(bucket)):\n",
    "                for j in np.arange(start=i+1, stop = len(bucket), step=1):\n",
    "                    pair = [bucket[i],bucket[int(j)]]\n",
    "                    if pair not in pairs:\n",
    "                        similarity = simi(pair,set_list_total)\n",
    "                        pairs.append(pair)\n",
    "                        if similarity > Threshold :\n",
    "                            similar_users.append(pair)\n",
    "                            similarities.append(similarity)\n",
    "                    if time.time() - start_time > 300:\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    return similar_users , similarities, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    f.write('%s,%s\\n' % (item[0],item[1]))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "       \n",
    "candidates = hash_to_bucket(bands = 25, sig_M = signature_matrix)\n",
    "similar_users , similarities , pairs = bucket_to_simi(candidates,0.5, time.time())\n",
    "\n",
    "        \n",
    "if os.path.exists('results.txt'):\n",
    "    os.remove('results.txt')\n",
    "with open('results.txt','w') as f:\n",
    "    for item in similar_users:\n",
    "        f.write('%s,%s\\n' % (item[0],item[1]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Meng's testing\n",
    "\n",
    "#candidates = set().union(candidate_list[0],candidate_list[1])\n",
    "#candidate_list[0] | candidate_list[1]\n",
    "#candidates[0] == candidates[1]\n",
    "'''\n",
    "candidates_unique = candidates[0]\n",
    "for i in range(bands-1):\n",
    "    candidates_unique += [e for e in candidates[i+1] if e not in candidates_unique]\n",
    "candidates_unique\n",
    "''''\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Michael's testing\n",
    "\n",
    "row = np.array([0, 2, 2, 0, 1, 2])\n",
    "col = np.array([0, 0, 1, 2, 2, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "A = csc_matrix((data, (row, col)), shape=(len(row), len(col))).toarray()\n",
    "print(A)\n",
    "x = np.arange(len(row))\n",
    "np.random.shuffle(x)\n",
    "print(x)\n",
    "B = A[x,:]\n",
    "print(B)\n",
    "A[0,:] = (B!=0).argmax(axis=0)\n",
    "print(A)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
