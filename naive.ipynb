{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AiDM Assignment 2 - LSH for the Netflix data\n",
    "## Naive method - Time esitimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "First of all, we load in the data of approximately 103,703 users and the movies they watched among 17,770 movies in total. The first column gives the user IDs and the second column gives the movies IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading time =  0.22000908851623535 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "time0 = time.time() # record the starting time of loading part.\n",
    "data_origin = np.load('../user_movie.npy')\n",
    "loading_time = time.time() - time0     # calculate the time of this part.\n",
    "print('loading time = ',loading_time,'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As wee can see here, our loading process took about 0.21 second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List and calculate the similarities\n",
    "Theoretically, This naive algorithm consists of two parts. Firstly, we would like to make a movie set for every single user and then put them in to a list. Secondly, for each user's movie set, we compare it with another user's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list(data,n):\n",
    "    '''\n",
    "    The function is to generate a list that contains sets for users. Each set is the watched movies for one user.\n",
    "    data :\n",
    "    n : persent of dataset will be considered\n",
    "    '''\n",
    "    data_n_persent = data[0:int(len(data_origin)*n),:] # Drag out n persent from the entire dataset.\n",
    "    user_ID = np.unique(data_n_persent[:,0])      # Make a user ID array, every single ID will only show once.\n",
    "    user_movie_list = []                     # Prepare an empty watched movie list\n",
    "    for i in range(len(user_ID)):\n",
    "        # For every user, we go through the data and record the movies that he/she watched.\n",
    "        marker = data[:,0] == user_ID[i]\n",
    "        user_movie_list.append(set(data[marker][:,1]))   # Convert it into set in order to compare with others'\n",
    "    return user_movie_list  # will return the whole watched movies set for every user in only one list.\n",
    "\n",
    "\n",
    "def simi(set_list):\n",
    "    '''\n",
    "    This function is to calculate the similarities between users.\n",
    "    Similarity = intersenction / union\n",
    "    set_list : \n",
    "    '''\n",
    "    similarities = np.zeros(len(set_list)) # Prepare an np array for saving the value of similarities.\n",
    "    for i in range(len(set_list)):\n",
    "        intersection = set_list[0]&set_list[i]  # Use logical and to achieve the intersection\n",
    "        similarities[i] = len(intersection)/(len(set_list[0])+len(set_list[i])-len(intersection)) # Calculate sim\n",
    "    return similarities # will return a array contains similarities between the first user and any others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record running time\n",
    "We did a test on 1 persent of the entire dataset and recorded the running time of listing and calculating the similarities separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138.52079319953918 seconds\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()    # Record the starting point of listing process.\n",
    "set_1persent = generate_list(data_origin,0.01)\n",
    "list_time = time.time() - time1       #  Calculate the time of this part.\n",
    "print('listing time = ',list_time,'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time it took to generate a list, which contains movie set of users, from 1 persent of the dataset is about 138.52 seconds. Note that we chop out the first 1 persent of data based on rows rather than users which means the information of the last user in our test might not be completed but this issue will not affect our result of estimated time. Hence, we achieved a list consisting 1040 users' movie set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0317378044128418 seconds\n"
     ]
    }
   ],
   "source": [
    "time2 = time.time()       # Record the staring time of similarities calculation process.\n",
    "simi_1persent = simi(set_1persent)\n",
    "simi_time = time.time() - time2   # calculate the time of this part\n",
    "print('similarities calculating time = ',simi_time,'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our timer, the calculating process took about 0.032 to run. The number of calculations is as the same as the number of users as we compared the first user with others ( to simplify the code, we campared the first user with himself), which is 1040."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esitimate the total  time\n",
    "Then we estimated the total time by considering 3 parts of the entire algorithm. \n",
    "\n",
    "First of all, the time cost by loading data process is fixed, which is aroud 0.25 sec. \n",
    "\n",
    "Second, the listing part is a linear opertation. Our test was making a list on 1040 users from 103,703 users. Hence, the total estimated time of second part is supposed to be about 100 times of it for our test, which is 103,703/1040 * 138.52 sec.\n",
    "\n",
    "Third, the similarities part. The time of this part is proportional to the number of calculations. In our case, we calculated 1040 similarities. The total dataset needs 103,703 * 103,702 / 2 times calculations. Therefore, we multipiled our test time by (103,703 * 103,702 /2) / 1040."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time =  2.0595609208055614  d\n"
     ]
    }
   ],
   "source": [
    "num_users = len(set(data_origin[:,0]))   # The  total number of users\n",
    "num_users_1p = len(set_1persent)         # The numbers of users in our test\n",
    "opertaions_toal = num_users * (num_users - 1) / 2.   # Total calculations \n",
    "opertaions_1p = num_users_1p                         # Calculations of our test\n",
    "op_factor = opertaions_toal / opertaions_1p\n",
    "total_time = loading_time + list_time * 100. + simi_time * op_factor    # Add them up\n",
    "print('Total time = ', total_time / 60. / 60. / 24., ' d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88845"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data_origin[:,1])*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final result of total estimated time is about 2 days."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
